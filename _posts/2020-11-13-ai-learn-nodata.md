---
layout: single
type: post
comments: true
date: 2020-11-13 22:00:01 +0330
jdate: 1399-08-23
title: یادگیری ماشین با حداقل داده
author: aminyousefnejad
image: /img/ai-nodata/Rhinocorn.webp
categories:
    - تکنولوژی
tags:
    - یادگیری ماشین
    - هوش مصنوعی
---

یادگیری ماشین جز دستاورد های خیلی مهم چند سال اخیره . اینکه یه چیزایی به یه مدل هوش مصنوعی یاد بدی و بعدش اون بتونه تو دنیای واقعی اونارو تشخیص بده یا مسایل مشابه رو خودش حل کنه خیلی باحاله  . ولی در عمل یادگیری ماشین به اطلاعات خیلی زیادی احتیاج داره تا مثلا بتونه یه اسب یا هویج رو تشخیص بده . باید صدها یا هزاران عکس اسب بهش نشون بدی تا بالاخره بتونه وقتی یه عکس جدید بهشون نشون دادی  تشخیص بده که این اسبه یا نه . این روش یادگیری خیلی با نحوه یادگیری انسان متفاوته .

<div id="read-more"></div>

برای مثال ، بچه یه انسان به داده های خیلی کمتری حتی در حد یدونه احتیاج داره تا یک شی رو برای آخر عمرش تشخیص بده . در واقع ، بچه ها بعضی وقتا حتی به هیچ مثال و داده ای احتیاج ندارن تا یه چیزی رو تشخیص بدن . عکس یه اسب و کرگدن بهشون نشون بدین و بگین که اسب تک شاخ یه چیزی بین اسب و کرگدنه . اونا می تونن اولین باری که تصویر  این اسب افسانه ای رو می بینن تشخیصش بدن . 



![rhinocorn](/img/ai-nodata/unicorn.jpg)

امممم .... خیلی خب . 

حالا یه مقاله ای از دانشگاه واترلو (University of Waterloo in Ontario) منتشر شده که میگه هوش مصنوعی هم باید بتونه همچین کاری کنه . پروسسی که محققا بهش میگین فرصت - کمتر از یکبار ، "Less than one " -shot  . (معادل بهتری پیدا کردید تو کامنت بنویسید ) . هدف از این تکنیک اینکه ، یه مدل هوش مصنوعی باید بتونه دقیقا بیشتری از چیزی که یادگرفته و باهاش تمرین کرده قادر به تشخیص باشه . مثلا یدونه عکس از اسب و یدونه عکس از هویج نشونش بدید و بتونه از هزار تا عکسی که بهش نشون میدید اسب و هویج رو درست تشخیص بده . این خیلی قدم بزرگیه چون تو این  قضیه یادگیری ماشین و داده های مورد نیازش داره بزرگتر و غیرقابل دسترس تر میشه . 



## Less than one - shot چطور کار می کنه 

![image of sample 0 to 9 handwriting](/img/ai-nodata/10_MnistExamples.png)

محققا اول تجربه ای که با دیتا ست بینایی-کامیپوتر به اسم [MNIST](http://yann.lecun.com/exdb/mnist/)داشتن رو توضیح میدن . منیست دیتا ستی که حاوی ۶۰ هزار عکس آزمایشی اعداد ۰ تا ۹ با دست خط های مختلف هست .

تو مقاله قبلی در این حوزه محققای MIT تکنیکی معرفی کرده بودن که داده های خیلی عظیم رو تبدیل به داده های کوچکتر کنن. به عنوان یه ایده ، اونا اومدن و دیتاست منیست رو تبدیل به ۱۰ تا تصویر کردن. داده ها مهندسی شده و با دقت انتخاب شدن تا اطلاعات کافی دقیقا به اندازه خود دیتا ست به مدل هوش مصنوعی بدن . و نتیجه یادگیری مدل با این ۱۰ تا تصویر تقریبا مثل زمانی بود که مدل همه تصویرای منیست رو دیده باشه . 

اما محققا میخاستن که کم کردن داده های ورودی رو جلو تر ببرن و گفتن اگه ممکنه  ۶۰ هزار تا عکس رو تو ۱۰ تا جمع کنیم چرا نتونیم تا ۵ فشرده تر نکنیم ؟؟ 

تکنیکشون این بود که تصاویری درست کنن که چنتا عدد رو با هم ترکیب کنن با لیبل های هیبرید یا سافت به خورد مدل بدن . اگه به عدد 3 نگا کنید یه جورایی شبیه 8 هم هست اما اصلا شبیه 7 نیست . در واقع کار سافت لیبل ها اینکه ویژگی های مشترک رو پیدان کنن و به جا اینکه به ماشین بگن این تصویر عدد 3 هست . بهش میگن این تصویر ۶۰٪  عدد 3 ، ۳۰٪ عدد 8 ، و ۱۰٪ عدد 0 هست . 
![distilled form MNIST](/img/ai-nodata/distilled-mnist.png)


## محدودیت های این تکنیک چیه 

خب الآن تونستن که یه دیتاست ۶۰ هزار تایی رو به ۵ تا عکس کاهش بدن ، تا کجا میخوان ادامه بدن ؟ اصلا محدودیتی برای این تکنیک هست ؟ محدودیتی برای موضوعات هست ؟ 

در کمال تعجب ! به نظر میاد نه ! اگه سافت لیبل ها به دقت هر چه تمام تر مهندسی بشن ، حتی دوتا مثال هم میتونه در تئوری همون کارو انجام بده . " با دوتا نقطه می شه هزار یا ۱۰ هزار یا یه میلیون کلاس رو از هم جدا کرد ."

اونا این ایده رو با یکی از ساده ترین الگوریتم های هوش مصنوعی به نام k-nearst neighbors (kNN  که اشیا رو کلاس بندی میکنه توضیح میدن .)

برای فهمیدن اینکه kNN چطور کار میکنه ، کلاس بندی کردن میوه ها رو در نظر بگیرین . اگه میخوایین که یه مدل kNN رو  آموزش بدین که فرق سیب و پرتقال رو بفهمه ، اول باید ویژگی هایی رو انتخاب کنید که بر اساس اون به مدل ، میوه ها رو نشون میدین . احتمالا رنگ و وزن رو انتخاب کنید . خب پس برای هر سیب و پرتقالی که به kNN نشون میدین یک نقطه رو نمودار ترسیم میشه که محور ایکس  رنگ و Y وزن هست . بعد از اینکه kNN همه این نقاط روی نمودار ترسیم کرد یه خط مرزی بین نقاط سیب و پرتقال میکشه . اینکار نمودارو تقریبا تبدیل به دوتا کلاس میکنه . حالا الگوریتم تصمیم میگیره دیتای جدیدی که روی نمودار ترسیم میشه کدوم طرف خط باشه سیبه یا پرتقال . 

![Plotting apples (green and red dots ) and orange](/img/ai-nodata/chart.png)

پس اومدن یه سری دیتا ست با تکنیک  LO-shot  و سافت لیبل های دقیق ساختن و  به خورد یه مدل kNN دادن . نتیجه این بود که این الگوریتم تونست نمودار رو ترسیم و کلاس های بیشتری نسبت به داده های ورودیش ترسیم کنه .  هر کدوم از ناحیه های رنگی یه کلاس رو نمایش میدن که طرحای باحالی داره . 

![knn-ploting-wiht-loshot](/img/ai-nodata/knn-with-LO-shot.png)

البته که این تئوری ها یه سری محدودیت هایی دارن همین LO-shot باعث میشه الگوریتم هامون پیچیده تر بشن و کار ساختن سافت لیبل ها سخت میشه . و شبکه های عصبی کار رو پیچیده تر می کنن . 

در واقع این تحقیقات هنوز در مراحل اولیه س . ولی می تونه بازی رو تغییر بده . شاید باعث بشه عصر وحشتناک هوش مصنوعی ای که اسب و هویج رو اشتباه نگیره فرا برسه !! 

[منبع](https://www.technologyreview.com/2020/10/16/1010566/ai-machine-learning-with-tiny-data/)